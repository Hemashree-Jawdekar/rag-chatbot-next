ğŸš€ RAG Chatbot â€“ PDF-Powered Question Answering

Next.js + Google Gemini + Vector Search

A lightweight Retrieval-Augmented Generation (RAG) system built with Next.js App Router, Google Gemini API, and a simple in-memory vector store.
Users can upload PDFs â†’ extract text â†’ embed â†’ store â†’ chat with an AI assistant that answers questions from your documents.

ğŸ”— Live Demo

ğŸ‘‰ Hosted on Vercel:

https://your-vercel-link.vercel.app

ğŸ“˜ Project Overview

This application lets users:

ğŸ“„ Upload PDF documents

ğŸ§µ Extract text using pdf-parse

ğŸ§  Generate vector embeddings using Google Gemini Embeddings

ğŸ“š Store chunks & embeddings in a lightweight vector store

ğŸ’¬ Chat with an AI assistant using retrieval + Gemini

ğŸ” Perform semantic similarity search (cosine similarity)

ğŸ§© Architecture
1. Ingestion Flow (Upload PDF)

User uploads PDF to /api/upload

Extract text via pdf-parse

Split text into chunks

Generate embeddings with:

models/embedding-001


Save {chunk, embedding} pairs in local vector store

2. Retrieval Flow (Chat)

User sends a query

Embed query using Gemini embeddings

Compare query vector with stored vectors â†’ cosine similarity

Select top-K most relevant chunks

Send them to Gemini model:

gemini-2.5-flash


Generate a grounded, contextual answer

ğŸ—ï¸ Tech Stack
Function	Tech
Frontend	Next.js 14 (App Router)
PDF Extraction	pdf-parse
Embedding Model	Gemini models/embedding-001
Chat Model	Gemini gemini-1.5-flash
Vector Storage	In-memory cosine similarity
Runtime	Node.js
ğŸ“¦ Installation & Setup
1. Clone the Repo
git clone https://github.com/<your-username>/<your-repo>.git
cd <your-repo>

2. Install Dependencies
npm install

3. Environment Variables

Create .env.local in your project root:

GEMINI_API_KEY=your_google_api_key_here


Get your key from:
ğŸ”— https://aistudio.google.com/app/apikey

4. Start Dev Server
npm run dev


Your app runs at:
ğŸ‘‰ http://localhost:3000

ğŸ“‚ Folder Structure
app/
 â”œâ”€ api/
 â”‚   â”œâ”€ upload/route.js   â†’ PDF upload & embedding
 â”‚   â””â”€ chat/route.js     â†’ Chat endpoint
lib/
 â”œâ”€ pdf.js                â†’ PDF extraction helper
 â”œâ”€ embedding.js          â†’ Gemini embedding generator
 â”œâ”€ chunk.js              â†’ Text splitter
 â””â”€ vectorStore.js        â†’ Local vector DB
components/
 â””â”€ ChatUI.jsx            â†’ Chat UI
public/
 â””â”€ screenshots/          â†’ Images for README
README.md

ğŸ–¼ï¸ Screenshots & Demo
Upload Screen

Chat Screen

Demo GIF

(Add your actual files inside public/screenshots/)

ğŸ“ Important Notes

Vector store is in-memory â†’ resets after server restart

Upload supports multi-PDF indexing

Retrieval uses cosine similarity

For production, you can switch to:

Pinecone

Supabase Vector

ChromaDB

pgvector

ğŸš€ Future Improvements

Persistent vector database

UI streaming responses

PDF multi-page summarization

User auth + personal document stores
